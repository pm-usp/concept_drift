{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# My packages\n",
    "from source import parse_mxml as pm\n",
    "from source import log_representation as lr\n",
    "from source import plots as plts\n",
    "from source import drift_detection as dd\n",
    "from source import drift_localization as dl\n",
    "from source import offline_streaming_clustering as off_sc\n",
    "from source import online_streaming_clustering as on_sc\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.base import clone as sk_clone \n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insensitive_glob(pattern):\n",
    "    def either(c):\n",
    "        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n",
    "    return glob.glob(''.join(map(either, pattern)))\n",
    "\n",
    "def if_any(string, lista):\n",
    "    # If the string contains any of the values\n",
    "    # from the list 'lista'\n",
    "    for l in lista:\n",
    "        if l in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List log files\n",
    "# logs = insensitive_glob(\"../process_mining_datasets/*/*k.MXML\")\n",
    "logs = [x.replace('..\\\\', '../') for x in logs if \"2.5\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference objects and map them to strings in dict \n",
    "# used in further methods\n",
    "objects = {\n",
    "    \"representation\": {\n",
    "        \"binary\": lambda x: lr.get_binary_representation(lr.get_traces_as_tokens(x)),\n",
    "        \"frequency\": lambda x: lr.get_frequency_representation(lr.get_traces_as_tokens(x)),\n",
    "        \"transitions\": lambda x: lr.get_binary_transitions_representation(lr.get_traces_as_tokens(x))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change patterns and they supported representations\n",
    "\n",
    "binary_support = [\"cb\", \"cf\",  \"cm\", \"rp\", \"pm\", \"fr\"]\n",
    "frequency_support = binary_support + ['lp', 'cp', ]\n",
    "transitions_support = frequency_support + [\"pl\", \"cd\", \"sw\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Streaming Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../LoanApplications_Streaming/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))\n",
    "\n",
    "def parse_name_info(name):\n",
    "    x = name.split('\\\\')[-1].split('/')[-1]\n",
    "    x_ = my_split(x)\n",
    "    return {\n",
    "        \"tipo_mudanca\": x_[0].replace('new_', ''),\n",
    "        \"log_size\": int(x_[1]) * 1000 if int(x_[1]) != 7 else 7500,\n",
    "        'file_ref': x\n",
    "    }\n",
    "\n",
    "def read_file_and_run_streaming_cluster_pipeline(args, return_result=False):\n",
    "    \"\"\"\n",
    "    Read an event log file, represent it into a feature vector space and\n",
    "    run the trace clustering method over windows. This method outputs results\n",
    "    as gzip csv files into the \"OUTPUT_PATH\" folder, or return the result \n",
    "    as DataFrame when return_result = True.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        args (dict): Dictionary with the parameters and the log_file path\n",
    "            requiring the following keys:\n",
    "                example = {\n",
    "                    'log': <PATH TO LOG_FILE>,\n",
    "                    'representation': <KEY TO REPRESENTATIONS IN 'objects'>,\n",
    "                    'parameters': [{\n",
    "                        'model': <KEY TO MODEL IN 'objects'>, \n",
    "                        'sliding_window': <WHETHER TO USE SLIDING WINDOW>,\n",
    "                        'window_size': <SIZE OF TRACE WINDOW TO USE>,\n",
    "                        'sliding_step': <STEP OF SLIDING WINDOW>\n",
    "                    }\n",
    "        return_result (bool): Whether to return the result as DataFrame\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pm.all_prep(open(args['log']))\n",
    "    df_ = objects['representation'][args['representation']](df)\n",
    "    \n",
    "    resp = parse_name_info(args['log'])\n",
    "\n",
    "    resp['input_log'] = args['log']\n",
    "    resp['representation'] = args['representation']\n",
    "    \n",
    "    output = OUTPUT_PATH + resp['tipo_mudanca'] + '/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for comb in args['parameters']:\n",
    "        r = copy.deepcopy(resp)\n",
    "        r.update(comb)\n",
    "        \n",
    "        p = {}\n",
    "        for k in comb:\n",
    "            if k.startswith('model_parameters__'):\n",
    "                p[k.replace('model_parameters__', '')] = comb[k]\n",
    "\n",
    "        output_file = output + '__'.join([\n",
    "            r['tipo_mudanca'],\n",
    "            str(r['log_size']),\n",
    "            r['representation'],\n",
    "            r['model']\n",
    "        ] + [str(key)+\"=\"+str(p[key]) for key in p])\n",
    "        \n",
    "        if os.path.isfile(output_file + '--results.pickle.gzip'):\n",
    "            print(\"Already exists\")\n",
    "            continue\n",
    "        \n",
    "        print(output_file + \"...\")\n",
    "        ret = on_sc.run_online_streaming_clustering(\n",
    "            model_name=comb['model'],\n",
    "            model_parameters=p,\n",
    "            df=df_,\n",
    "            use_tqdm=False,\n",
    "            get_macro_clusters=True\n",
    "        )\n",
    "        \n",
    "        ft = on_sc.extract_features_dataframe(ret, use_tqdm=False)\n",
    "        \n",
    "        for key in r:\n",
    "            ret[key] = r[key]\n",
    "        \n",
    "        ret.to_pickle(output_file + '--results.pickle.gzip', compression=\"gzip\")\n",
    "        ft.to_pickle(output_file + '--features.pickle.gzip', compression=\"gzip\")\n",
    "\n",
    "        if return_result:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run pipeline for specific case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3...\n"
     ]
    }
   ],
   "source": [
    "all_metrics = read_file_and_run_streaming_cluster_pipeline({\n",
    "    'log': logs[0],\n",
    "    'representation': 'binary',\n",
    "    'parameters': [{\n",
    "        'model': 'MOACluStream',\n",
    "        'model_parameters__h': 150,\n",
    "        'model_parameters__m': 30,\n",
    "        'model_parameters__t': 2,\n",
    "        'model_parameters__k_macro': 3,\n",
    "    }]\n",
    "}, return_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_names</th>\n",
       "      <th>micro</th>\n",
       "      <th>micro_ids</th>\n",
       "      <th>micro_radius</th>\n",
       "      <th>micro_weights</th>\n",
       "      <th>outliers</th>\n",
       "      <th>outliers_ids</th>\n",
       "      <th>outliers_radius</th>\n",
       "      <th>outliers_weights</th>\n",
       "      <th>macro</th>\n",
       "      <th>...</th>\n",
       "      <th>tipo_mudanca</th>\n",
       "      <th>log_size</th>\n",
       "      <th>file_ref</th>\n",
       "      <th>input_log</th>\n",
       "      <th>representation</th>\n",
       "      <th>model</th>\n",
       "      <th>model_parameters__h</th>\n",
       "      <th>model_parameters__m</th>\n",
       "      <th>model_parameters__t</th>\n",
       "      <th>model_parameters__k_macro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Appraise_property, Approve_application, Asses...</td>\n",
       "      <td>[[1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2e-25]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>...</td>\n",
       "      <td>pm</td>\n",
       "      <td>10000</td>\n",
       "      <td>pm10k.mxml</td>\n",
       "      <td>/Users/rafaelgaspar/Library/CloudStorage/Googl...</td>\n",
       "      <td>binary</td>\n",
       "      <td>MOACluStream</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Appraise_property, Approve_application, Asses...</td>\n",
       "      <td>[[1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[2e-25, 2e-25]</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>...</td>\n",
       "      <td>pm</td>\n",
       "      <td>10000</td>\n",
       "      <td>pm10k.mxml</td>\n",
       "      <td>/Users/rafaelgaspar/Library/CloudStorage/Googl...</td>\n",
       "      <td>binary</td>\n",
       "      <td>MOACluStream</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               col_names  \\\n",
       "index                                                      \n",
       "0      [Appraise_property, Approve_application, Asses...   \n",
       "1      [Appraise_property, Approve_application, Asses...   \n",
       "\n",
       "                                                   micro micro_ids  \\\n",
       "index                                                                \n",
       "0      [[1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,...       [0]   \n",
       "1      [[1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,...    [0, 1]   \n",
       "\n",
       "         micro_radius micro_weights  outliers outliers_ids  outliers_radius  \\\n",
       "index                                                                         \n",
       "0             [2e-25]         [1.0]       NaN           []              NaN   \n",
       "1      [2e-25, 2e-25]    [1.0, 1.0]       NaN           []              NaN   \n",
       "\n",
       "       outliers_weights                                              macro  \\\n",
       "index                                                                        \n",
       "0                   NaN  [[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...   \n",
       "1                   NaN  [[1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...   \n",
       "\n",
       "       ... tipo_mudanca log_size    file_ref  \\\n",
       "index  ...                                     \n",
       "0      ...           pm    10000  pm10k.mxml   \n",
       "1      ...           pm    10000  pm10k.mxml   \n",
       "\n",
       "                                               input_log  representation  \\\n",
       "index                                                                      \n",
       "0      /Users/rafaelgaspar/Library/CloudStorage/Googl...          binary   \n",
       "1      /Users/rafaelgaspar/Library/CloudStorage/Googl...          binary   \n",
       "\n",
       "              model  model_parameters__h  model_parameters__m  \\\n",
       "index                                                           \n",
       "0      MOACluStream                  150                   30   \n",
       "1      MOACluStream                  150                   30   \n",
       "\n",
       "       model_parameters__t  model_parameters__k_macro  \n",
       "index                                                  \n",
       "0                        2                          3  \n",
       "1                        2                          3  \n",
       "\n",
       "[2 rows x 47 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments with several parameters combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 18, 1350)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_logs = list(ParameterGrid([\n",
    "    {\n",
    "        \"log\": logs,\n",
    "        \"representation\": ['binary', 'frequency', 'transitions']\n",
    "    }\n",
    "]))\n",
    "\n",
    "grid_parameters = list(ParameterGrid({\n",
    "    'model': ['MOACluStream'],\n",
    "    'model_parameters__h': [50, 150, 200],\n",
    "    'model_parameters__m': [30, 50],\n",
    "    'model_parameters__t': [2],\n",
    "    'model_parameters__k_macro': [2, 3],\n",
    "}))\n",
    "\n",
    "combs = []\n",
    "for x in grid_logs:\n",
    "    dic = x.copy()\n",
    "    dic['parameters'] = grid_parameters\n",
    "    \n",
    "    combs.append(dic)\n",
    "len(combs), len(grid_parameters), len(combs) * len(grid_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cc7db693d948639c9887d51f091c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_resp = Parallel(n_jobs=3)(\n",
    "    delayed(read_file_and_run_clustering_pipeline)(comb) for comb in tqdm_notebook(combs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection parameters\n",
    "drift_config = list(ParameterGrid([\n",
    "    {\n",
    "        \"rolling_window\": [50, 100, 150, 200],\n",
    "        \"std_tolerance\": [1.25, 1.5, 1.75, 2],\n",
    "        'min_tol': [0.01, 0.03] \n",
    "    }\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files obtained after the trace clustering pipeline\n",
    "\n",
    "clusterizacoes = glob.glob(OUTPUT_PATH + \"*/*.pickle.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3--features.pickle.gzip',\n",
       " 'LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3--results.pickle.gzip']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterizacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations to run\n",
    "\n",
    "combs_deteccao = []\n",
    "for log in clusterizacoes:\n",
    "    combs_deteccao.append({\n",
    "        'input': log,\n",
    "        'combinations': drift_config\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace part of the OUTPUT_PATH to create a new folder \n",
    "# for the detection results\n",
    "\n",
    "OLD_prefix = \"LoanApplications_Streaming\"\n",
    "NEW_prefix = \"LoanApplications_Streaming__DETECTION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detect_pipeline(args, return_results=False):\n",
    "    all_metrics = pd.read_pickle(args[\"input\"], compression='gzip')\n",
    "#     all_metrics['sum_diff_centroids'] = all_metrics['diff_centroids'].apply(\n",
    "#         lambda x: np.abs(x.sum(axis=0)).sum() if not x is np.nan and x.shape[0] > 1 else np.nan\n",
    "#     )\n",
    "    \n",
    "    path_file = args[\"input\"].replace(\"--results.pickle.gzip\", \"\").split('\\\\')\n",
    "    \n",
    "    args.update({\n",
    "        \"tipo_mudanca\": path_file[-1].split('__')[0],\n",
    "        \"log_size\": int(path_file[-1].split('__')[1]),\n",
    "        \"model\": path_file[-1].split('__')[3],\n",
    "        \"representation\": path_file[-1].split('__')[2],\n",
    "    })\n",
    "    \n",
    "    #unpack model parameters\n",
    "    for model_p in path_file[-1].split('__')[4:]:\n",
    "        args[model_p.split('=')[0]] = model_p.split('=')[1]\n",
    "    \n",
    "    \n",
    "    for combination in args['combinations']:\n",
    "        c = deepcopy(combination)\n",
    "        c.update(args)\n",
    "        \n",
    "        __drift_detect_pipeline(\n",
    "            all_metrics, c, return_results=return_results\n",
    "        )\n",
    "    \n",
    "\n",
    "def __drift_detect_pipeline(all_metrics, args, return_results=False):     \n",
    "    base_name = args[\"input\"].replace(\".pickle.gzip\", \"\")\n",
    "    base_name = base_name.replace(\"EXEC__LoanApplications_STREAMING\", \"EXEC__LoanApplications_STREAMING__DETECTION\")\n",
    "    \n",
    "    to_string = [\n",
    "        str(args[\"rolling_window\"]),\n",
    "        str(args[\"std_tolerance\"]).replace(\".\", \"-\"), \n",
    "        str(args[\"min_tol\"]).replace(\".\", \"-\")\n",
    "    ]\n",
    "    \n",
    "    if \"smooth_factor\" in args:\n",
    "        to_string += [str(args[\"smooth_factor\"]).replace(\".\", \"-\")]\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(base_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    final_name = base_name.replace(\"\\\\\", \"/\") + \"/\" + \"_\".join(to_string) + \".pickle.gzip\"\n",
    "    \n",
    "    if os.path.isfile(final_name):\n",
    "        try:\n",
    "            x = pd.read_pickle(final_name, compression='gzip')\n",
    "            print(\"Already exists\")\n",
    "            return\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # all_metrics = pd.read_csv(args[\"input\"], index_col=0)\n",
    "    \n",
    "    # print(info)\n",
    "    # Monta o vetor de resposta\n",
    "    y_true = [x for x in range(int(args['log_size']/10), args['log_size'], int(args['log_size']/10))]\n",
    "    \n",
    "    results = []\n",
    "    for col in all_metrics.select_dtypes(include=np.number).columns:\n",
    "        if (col not in [\"k\"] and not col.startswith(\"diff\") ) or col in [\"diff_centroids\"]:\n",
    "            r = deepcopy(args)\n",
    "            r[\"measure\"] = col\n",
    "\n",
    "            # print(col)\n",
    "            # print(all_metrics[col].dtype)\n",
    "\n",
    "            detected_drifts, extra = dd.detect_concept_drift(\n",
    "                all_metrics, \n",
    "                col,\n",
    "                args[\"rolling_window\"],\n",
    "                args[\"std_tolerance\"],\n",
    "                args[\"min_tol\"]\n",
    "            )\n",
    "\n",
    "            metrics_results = dd.get_metrics(\n",
    "                detected_drifts,\n",
    "                y_true,\n",
    "                int(args[\"h\"])\n",
    "            )\n",
    "\n",
    "            r.update(args)\n",
    "            r.update(metrics_results)\n",
    "\n",
    "            results.append(r)\n",
    "\n",
    "            gc.collect()\n",
    "    \n",
    "    print(final_name)\n",
    "    ret = pd.DataFrame(results)\n",
    "    ret.to_pickle(\n",
    "        final_name,\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "\n",
    "    if return_results:\n",
    "        return ret\n",
    "    # print(col, len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3--features/3_1-25_0-01.pickle.gzip\n",
      "LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3--features/3_1-25_0-02.pickle.gzip\n"
     ]
    }
   ],
   "source": [
    "detection_results = drift_detect_pipeline({\n",
    "    'input': 'LoanApplications_Streaming/pm/pm__10000__binary__MOACluStream__h=150__m=30__t=2__k_macro=3--features.pickle.gzip',\n",
    "    'combinations': [{\n",
    "       'min_tol': 0.01,\n",
    "       'rolling_window': 3,\n",
    "       'std_tolerance': 1.25\n",
    "    }, {\n",
    "       'min_tol': 0.02,\n",
    "       'rolling_window': 3,\n",
    "       'std_tolerance': 1.25\n",
    "    }]\n",
    "}, return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results.sort_values('F1', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a21195ec844156b2a711a25010295e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=54.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_resp = Parallel(n_jobs=3)(\n",
    "    delayed(drift_detect_pipeline)(comb_d) for comb_d in tqdm_notebook(combs_deteccao)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
